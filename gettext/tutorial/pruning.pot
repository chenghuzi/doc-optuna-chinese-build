# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, Optuna Contributors.
# This file is distributed under the same license as the Optuna package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Optuna 1.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-06-17 19:47-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/tutorial/pruning.rst:4
msgid "Pruning Unpromising Trials"
msgstr ""

#: ../../source/tutorial/pruning.rst:6
msgid "This feature automatically stops unpromising trials at the early stages of the training (a.k.a., automated early-stopping). Optuna provides interfaces to concisely implement the pruning mechanism in iterative training algorithms."
msgstr ""

#: ../../source/tutorial/pruning.rst:11
msgid "Activating Pruners"
msgstr ""

#: ../../source/tutorial/pruning.rst:12
msgid "To turn on the pruning feature, you need to call :func:`~optuna.trial.Trial.report` and :func:`~optuna.trial.Trial.should_prune` after each step of the iterative training. :func:`~optuna.trial.Trial.report` periodically monitors the intermediate objective values. :func:`~optuna.trial.Trial.should_prune` decides termination of the trial that does not meet a predefined condition."
msgstr ""

#: ../../source/tutorial/pruning.rst:53
msgid "Executing the script above:"
msgstr ""

#: ../../source/tutorial/pruning.rst:70
msgid "``Trial 5 pruned.``, etc. in the log messages means several trials were stopped before they finished all of the iterations."
msgstr ""

#: ../../source/tutorial/pruning.rst:74
msgid "Integration Modules for Pruning"
msgstr ""

#: ../../source/tutorial/pruning.rst:75
msgid "To implement pruning mechanism in much simpler forms, Optuna provides integration modules for the following libraries."
msgstr ""

#: ../../source/tutorial/pruning.rst:77
msgid "XGBoost: :class:`optuna.integration.XGBoostPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:78
msgid "LightGBM: :class:`optuna.integration.LightGBMPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:79
msgid "Chainer: :class:`optuna.integration.ChainerPruningExtension`"
msgstr ""

#: ../../source/tutorial/pruning.rst:80
msgid "Keras: :class:`optuna.integration.KerasPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:81
msgid "TensorFlow :class:`optuna.integration.TensorFlowPruningHook`"
msgstr ""

#: ../../source/tutorial/pruning.rst:82
msgid "tf.keras :class:`optuna.integration.TFKerasPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:83
msgid "MXNet :class:`optuna.integration.MXNetPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:84
msgid "PyTorch Ignite :class:`optuna.integration.PyTorchIgnitePruningHandler`"
msgstr ""

#: ../../source/tutorial/pruning.rst:85
msgid "PyTorch Lightning :class:`optuna.integration.PyTorchLightningPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:86
msgid "FastAI :class:`optuna.integration.FastAIPruningCallback`"
msgstr ""

#: ../../source/tutorial/pruning.rst:88
msgid "For example, :class:`~optuna.integration.XGBoostPruningCallback` introduces pruning without directly changing the logic of training iteration. (See also `example <https://github.com/optuna/optuna/blob/master/examples/pruning/xgboost_integration.py>`_ for the entire script.)"
msgstr ""
